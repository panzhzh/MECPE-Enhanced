# Large Model Configuration for MECPE-Enhanced
# Using RoBERTa-large for better performance

# Data Configuration
data:
  data_root: "data"
  ecf_train_file: "train.txt"
  ecf_dev_file: "dev.txt"
  ecf_test_file: "test.txt"
  meld_root: "data/MELD.Raw"
  ecf_meld_mapping: "data/all_data_pair_ECFvsMELD.txt"
  
  # Text processing
  max_seq_length: 128
  max_doc_length: 75

# Model Configuration  
model:
  # Text encoder (系统会自动检测1024维度)
  text_model: "roberta-large"
  
  # Multimodal fusion
  fusion_hidden_size: 512  # 更大的融合层
  fusion_dropout: 0.1
  
  # Task-specific
  num_emotions: 6
  num_labels: 2

# Training Configuration (针对large模型调整)
training:
  batch_size: 4         # 更小batch size (large模型内存占用大)
  learning_rate: 1e-5   # 更小学习率 (large模型更敏感)
  num_epochs: 5         # 更少epoch (large模型收敛快)
  warmup_steps: 200
  weight_decay: 0.01
  gradient_clip_val: 1.0
  
  # Hardware
  device: "auto"
  mixed_precision: true  # 混合精度节省内存
  
  # Checkpointing
  save_dir: "experiments/checkpoints_large"
  log_every: 25
  eval_every: 200

# Experiment Configuration
experiment:
  name: "mecpe_large_baseline"
  seed: 42
  
  # Logging
  use_wandb: false